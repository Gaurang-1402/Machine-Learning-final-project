{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.io import arff\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HiddenLayer:\n",
    "    def __init__(self, initialWeights, initialBias, activationFunction):\n",
    "        self.w = initialWeights # weights\n",
    "        self.b = initialBias    # bias\n",
    "        self.i = None           # input\n",
    "        self.z = None           # output\n",
    "        self.d = None           # delta\n",
    "        self.a = None\n",
    "        self.actFunc = activationFunction\n",
    "    \n",
    "    def forward(self, input):\n",
    "        self.i = input\n",
    "        self.z = np.dot(input, self.w.T) + self.b\n",
    "        self.a = self.actFunc.f(self.z)\n",
    "    \n",
    "    def backward(self, nextLayerDelta, nextLayerWeights):\n",
    "        self.d = np.dot(nextLayerDelta, nextLayerWeights) * self.actFunc.f_prime(self.z)\n",
    "    \n",
    "    def update(self, learningRate, lam):\n",
    "        self.w -= learningRate * (np.dot(self.d.T, self.i) + lam * self.w)\n",
    "        self.b -= learningRate * self.d.sum(axis=0)\n",
    "\n",
    "class OutputLayer(HiddenLayer):\n",
    "    def __init__(self, initialWeights, initialBias, activationFunction):\n",
    "        super().__init__(initialWeights, initialBias, activationFunction)\n",
    "    \n",
    "    def backward(self, target):\n",
    "        self.d = (self.a - target) * self.actFunc.f_prime(self.z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationSigmoid:\n",
    "    def f(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def f_prime(self, x):\n",
    "        return self.f(x) * (1 - self.f(x))\n",
    "    \n",
    "class ActivationReLU:\n",
    "    def f(self, x):\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def f_prime(self, x):\n",
    "        return np.where(x > 0, 1, 0)\n",
    "\n",
    "class ActivationTanh:\n",
    "    def f(self, x):\n",
    "        return np.tanh(x)\n",
    "    \n",
    "    def f_prime(self, x):\n",
    "        return 1 - np.tanh(x) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, layers, learningRate):\n",
    "        self.layers = layers\n",
    "        self.learningRate = learningRate\n",
    "        self.cost_over_time_training = None\n",
    "        self.accuracy_over_time_training = None\n",
    "        self.cost_over_time_validation = None\n",
    "        self.accuracy_over_time_validation = None\n",
    "    \n",
    "    def forward(self, input):\n",
    "        for layer in self.layers:\n",
    "            layer.forward(input)\n",
    "            input = layer.a\n",
    "    \n",
    "    def backward(self, target):\n",
    "        for i in reversed(range(len(self.layers))):\n",
    "            if i == len(self.layers) - 1:\n",
    "                self.layers[i].backward(target)\n",
    "            else:\n",
    "                self.layers[i].backward(self.layers[i+1].d, self.layers[i+1].w)\n",
    "    \n",
    "    def update(self, learningRate, lam):\n",
    "        for layer in self.layers:\n",
    "            layer.update(learningRate, lam)\n",
    "    \n",
    "    def predict(self, input):\n",
    "        self.forward(input)\n",
    "        return self.layers[-1].a\n",
    "    \n",
    "    def cost(self, target, lam):\n",
    "        mse = (0.5 / target.shape[0]) * np.sum((target - self.layers[-1].a)**2)\n",
    "        l2_reg = 0.5 * lam * sum(np.sum(layer.w**2) for layer in self.layers[:-1])  # Exclude output layer\n",
    "        return mse + l2_reg\n",
    "    \n",
    "    def accuracy(self, X, y):\n",
    "        predictions = np.argmax(self.predict(X), axis=1)\n",
    "        y_vals = np.argmax(y, axis=1)\n",
    "        return np.mean(predictions == y_vals)\n",
    "    \n",
    "    def train(self, X, y, epochs, lam, X_val=None, y_val=None):\n",
    "        self.cost_over_time_training = []\n",
    "        self.accuracy_over_time_training = []\n",
    "        if X_val is not None and y_val is not None:\n",
    "            self.cost_over_time_validation = []\n",
    "            self.accuracy_over_time_validation = []\n",
    "        for _ in range(epochs):\n",
    "            self.forward(X)\n",
    "            self.backward(y)\n",
    "            self.update(self.learningRate, lam)\n",
    "            self.cost_over_time_training.append(self.cost(y, lam))\n",
    "            self.accuracy_over_time_training.append(self.accuracy(X, y))\n",
    "            if X_val is not None and y_val is not None:\n",
    "                self.forward(X_val)\n",
    "                self.cost_over_time_validation.append(self.cost(y_val, lam))\n",
    "                self.accuracy_over_time_validation.append(self.accuracy(X_val, y_val))\n",
    "    \n",
    "    def graph_cost_over_time(self):\n",
    "        # plots the cost over time for training and validation together\n",
    "        plt.plot(self.cost_over_time_training, label=\"Training\")\n",
    "        if self.cost_over_time_validation is not None:\n",
    "            plt.plot(self.cost_over_time_validation, label=\"Validation\")\n",
    "        plt.title(\"Cost over time\")\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Cost\")\n",
    "        plt.legend()\n",
    "        # show precise value for first and last epoch\n",
    "        plt.annotate(str(round(self.cost_over_time_training[0], 4)), xy=(0, self.cost_over_time_training[0]))\n",
    "        plt.annotate(str(round(self.cost_over_time_training[-1], 4)), xy=(len(self.cost_over_time_training)-1, self.cost_over_time_training[-1]))\n",
    "        if self.cost_over_time_validation is not None:\n",
    "            plt.annotate(str(round(self.cost_over_time_validation[0], 4)), xy=(0, self.cost_over_time_validation[0]))\n",
    "            plt.annotate(str(round(self.cost_over_time_validation[-1], 4)), xy=(len(self.cost_over_time_validation)-1, self.cost_over_time_validation[-1]))\n",
    "        plt.show()\n",
    "        \n",
    "    def graph_accuracy_over_time(self):\n",
    "        plt.plot(self.accuracy_over_time_training, label=\"Training\")\n",
    "        if self.accuracy_over_time_validation is not None:\n",
    "            plt.plot(self.accuracy_over_time_validation, label=\"Validation\")\n",
    "        plt.title(\"Accuracy over time\")\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Accuracy\")\n",
    "        plt.legend()\n",
    "        # show precise value for first and last epoch\n",
    "        plt.annotate(str(round(self.accuracy_over_time_training[0], 4)), xy=(0, self.accuracy_over_time_training[0]))\n",
    "        plt.annotate(str(round(self.accuracy_over_time_training[-1], 4)), xy=(len(self.accuracy_over_time_training)-1, self.accuracy_over_time_training[-1]))\n",
    "        if self.accuracy_over_time_validation is not None:\n",
    "            plt.annotate(str(round(self.accuracy_over_time_validation[0], 4)), xy=(0, self.accuracy_over_time_validation[0]))\n",
    "            plt.annotate(str(round(self.accuracy_over_time_validation[-1], 4)), xy=(len(self.accuracy_over_time_validation)-1, self.accuracy_over_time_validation[-1]))\n",
    "        plt.show()\n",
    "    \n",
    "    def graph_cost_and_accuracy_over_time(self):\n",
    "        # graph cost and accuracy next to each other\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        ax1.plot(self.cost_over_time_training, label=\"Training\")\n",
    "        if self.cost_over_time_validation is not None:\n",
    "            ax1.plot(self.cost_over_time_validation, label=\"Validation\")\n",
    "        ax1.set_title(\"Cost over time\")\n",
    "        ax1.set_xlabel(\"Epochs\")\n",
    "        ax1.set_ylabel(\"Cost\")\n",
    "        ax1.legend()\n",
    "        # show precise value for first and last epoch\n",
    "        ax1.annotate(str(round(self.cost_over_time_training[0], 4)), xy=(0, self.cost_over_time_training[0]))\n",
    "        ax1.annotate(str(round(self.cost_over_time_training[-1], 4)), xy=(len(self.cost_over_time_training)-1, self.cost_over_time_training[-1]))\n",
    "        if self.cost_over_time_validation is not None:\n",
    "            ax1.annotate(str(round(self.cost_over_time_validation[0], 4)), xy=(0, self.cost_over_time_validation[0]))\n",
    "            ax1.annotate(str(round(self.cost_over_time_validation[-1], 4)), xy=(len(self.cost_over_time_validation)-1, self.cost_over_time_validation[-1]))\n",
    "        \n",
    "        ax2.plot(self.accuracy_over_time_training, label=\"Training\")\n",
    "        if self.accuracy_over_time_validation is not None:\n",
    "            ax2.plot(self.accuracy_over_time_validation, label=\"Validation\")\n",
    "        ax2.set_title(\"Accuracy over time\")\n",
    "        ax2.set_xlabel(\"Epochs\")\n",
    "        ax2.set_ylabel(\"Accuracy\")\n",
    "        ax2.legend()\n",
    "        # show precise value for first and last epoch\n",
    "        ax2.annotate(str(round(self.accuracy_over_time_training[0], 4)), xy=(0, self.accuracy_over_time_training[0]))\n",
    "        ax2.annotate(str(round(self.accuracy_over_time_training[-1], 4)), xy=(len(self.accuracy_over_time_training)-1, self.accuracy_over_time_training[-1]))\n",
    "        if self.accuracy_over_time_validation is not None:\n",
    "            ax2.annotate(str(round(self.accuracy_over_time_validation[0], 4)), xy=(0, self.accuracy_over_time_validation[0]))\n",
    "            ax2.annotate(str(round(self.accuracy_over_time_validation[-1], 4)), xy=(len(self.accuracy_over_time_validation)-1, self.accuracy_over_time_validation[-1]))\n",
    "        \n",
    "        plt.show()\n",
    "    \n",
    "    def print(self):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            print(\"Layer\", i)\n",
    "            print(\"Weights:\\n\", layer.w)\n",
    "            print(\"Bias:\\n\", layer.b)\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BuildNetwork(structure, activation_function, initialization=np.zeros, learning_rate=0.1):\n",
    "    layers = []\n",
    "    for i in range(len(structure)-2):\n",
    "        layers.append(HiddenLayer(\n",
    "            initialization(structure[i+1], structure[i]),\n",
    "            initialization(1, structure[i+1]),\n",
    "            activation_function\n",
    "        ))\n",
    "    layers.append(OutputLayer(\n",
    "        initialization(structure[-1], structure[-2]),\n",
    "        initialization(1, structure[-1]),\n",
    "        activation_function\n",
    "    ))\n",
    "\n",
    "    return NeuralNetwork(layers, learning_rate)\n",
    "\n",
    "def ZerosInitialization(m, n):\n",
    "    return np.zeros((m, n))\n",
    "\n",
    "def UniformInitialization(m, n):\n",
    "    limit = np.sqrt(6 / (m + n))\n",
    "    return np.random.uniform(-limit, limit, (m, n))\n",
    "\n",
    "def NormalInitialization(m, n):\n",
    "    std = np.sqrt(2 / (m + n))\n",
    "    return np.random.normal(0, std, (m, n))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_from_each_class: 6449\n",
      "X_train: (18057, 42)\n",
      "y_train: (18057, 3)\n",
      "X_test: (6755, 42)\n",
      "y_test: (6755, 3)\n",
      "X_val: (6757, 42)\n",
      "y_val: (6757, 3)\n"
     ]
    }
   ],
   "source": [
    "loaded = arff.loadarff('../connect-4.arff')\n",
    "\n",
    "data = np.asarray(loaded[0].tolist(), dtype=np.float32)\n",
    "X = data[:, :-1]\n",
    "y = data[:, -1]\n",
    "\n",
    "# extract the classes\n",
    "X_zeros = X[y == 0]\n",
    "y_zeros = y[y == 0]\n",
    "X_ones = X[y == 1]\n",
    "y_ones = y[y == 1]\n",
    "X_twos = X[y == 2]\n",
    "y_twos = y[y == 2]\n",
    "\n",
    "max_from_each_class = min(X_zeros.shape[0], X_ones.shape[0], X_twos.shape[0])\n",
    "print(\"max_from_each_class:\", max_from_each_class)\n",
    "\n",
    "# shuffle\n",
    "np.random.seed(10)\n",
    "X_zeros, y_zeros = shuffle(X_zeros, y_zeros)\n",
    "X_ones, y_ones = shuffle(X_ones, y_ones)\n",
    "X_twos, y_twos = shuffle(X_twos, y_twos)\n",
    "\n",
    "# take only the first max_from_each_class elements\n",
    "X_zeros = X_zeros[:max_from_each_class]\n",
    "y_zeros = y_zeros[:max_from_each_class]\n",
    "X_ones = X_ones[:max_from_each_class]\n",
    "y_ones = y_ones[:max_from_each_class]\n",
    "X_twos = X_twos[:max_from_each_class]\n",
    "y_twos = y_twos[:max_from_each_class]\n",
    "\n",
    "# split into train, test, and validation\n",
    "X_zeros_train, X_zeros_testval, y_zeros_train, y_zeros_testval = train_test_split(X_zeros, y_zeros, test_size=0.2)\n",
    "X_ones_train, X_ones_testval, y_ones_train, y_ones_testval = train_test_split(X_ones, y_ones, test_size=0.2)\n",
    "X_twos_train, X_twos_testval, y_twos_train, y_twos_testval = train_test_split(X_twos, y_twos, test_size=0.2)\n",
    "X_zeros_test, X_zeros_val, y_zeros_test, y_zeros_val = train_test_split(X_zeros_testval, y_zeros_testval, test_size=0.5)\n",
    "X_ones_test, X_ones_val, y_ones_test, y_ones_val = train_test_split(X_ones_testval, y_ones_testval, test_size=0.5)\n",
    "X_twos_test, X_twos_val, y_twos_test, y_twos_val = train_test_split(X_twos_testval, y_twos_testval, test_size=0.5)\n",
    "\n",
    "# concatenate\n",
    "X_train = np.concatenate((X_zeros_train[:max_from_each_class], X_ones_train[:max_from_each_class], X_twos_train[:max_from_each_class]), axis=0)\n",
    "y_train = np.concatenate((y_zeros_train[:max_from_each_class], y_ones_train[:max_from_each_class], y_twos_train[:max_from_each_class]), axis=0)\n",
    "X_test = np.concatenate((X_zeros_test[:max_from_each_class], X_ones_test[:max_from_each_class], X_twos_test[:max_from_each_class]), axis=0)\n",
    "y_test = np.concatenate((y_zeros_test[:max_from_each_class], y_ones_test[:max_from_each_class], y_twos_test[:max_from_each_class]), axis=0)\n",
    "X_val = np.concatenate((X_zeros_val[:max_from_each_class], X_ones_val[:max_from_each_class], X_twos_val[:max_from_each_class]), axis=0)\n",
    "y_val = np.concatenate((y_zeros_val[:max_from_each_class], y_ones_val[:max_from_each_class], y_twos_val[:max_from_each_class]), axis=0)\n",
    "\n",
    "# one hot encoding\n",
    "y_train = np.eye(3)[y_train.astype(int)]\n",
    "y_test = np.eye(3)[y_test.astype(int)]\n",
    "y_val = np.eye(3)[y_val.astype(int)]\n",
    "# convert 2s to -1s\n",
    "X_train[X_train == 2] = -1\n",
    "X_test[X_test == 2] = -1\n",
    "X_val[X_val == 2] = -1\n",
    "\n",
    "print(\"X_train:\", X_train.shape)\n",
    "print(\"y_train:\", y_train.shape)\n",
    "print(\"X_test:\", X_test.shape)\n",
    "print(\"y_test:\", y_test.shape)\n",
    "print(\"X_val:\", X_val.shape)\n",
    "print(\"y_val:\", y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainNetworks(networks, epochs, lams=[0, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000]):\n",
    "    network_best_accuracy = [0 for _ in networks]\n",
    "    network_best_lambda = [0 for _ in networks]\n",
    "    for i, network in enumerate(networks):\n",
    "        print(\"Training network\", i+1)\n",
    "        for lam in lams:\n",
    "            print(\"Lambda:\", lam, end=\"... \")\n",
    "            network.train(X_train, y_train, epochs, lam, X_val, y_val)\n",
    "            acc = network.accuracy(X_val, y_val)\n",
    "            print(\"Accuracy:\", acc)\n",
    "            if acc > network_best_accuracy[i]:\n",
    "                network_best_accuracy[i] = acc\n",
    "                network_best_lambda[i] = lam\n",
    "        print(\"Best accuracy:\", network_best_accuracy[i], \"with lambda\", network_best_lambda[i], \"\\n\")\n",
    "        network.graph_cost_and_accuracy_over_time()\n",
    "    # get index of best accuracy\n",
    "    best_network_index = network_best_accuracy.index(max(network_best_accuracy))\n",
    "    print(\"Best network is network\", best_network_index+1, \"with accuracy\", network_best_accuracy[best_network_index], \"and lambda\", network_best_lambda[best_network_index])\n",
    "    return networks[best_network_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network 1\n",
      "Lambda: 0... Accuracy: 0.6917270978244783\n",
      "Lambda: 0.0001... Accuracy: 0.6859553056089981\n",
      "Lambda: 0.001... Accuracy: 0.6989788367618766\n",
      "Lambda: 0.01... "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[171], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[39mfor\u001b[39;00m initialization \u001b[39min\u001b[39;00m initializations:\n\u001b[0;32m      9\u001b[0m             networks\u001b[39m.\u001b[39mappend(BuildNetwork(layer, activation, initialization, \u001b[39m0.000005\u001b[39m))\n\u001b[1;32m---> 11\u001b[0m network \u001b[39m=\u001b[39m TrainNetworks(networks, \u001b[39m1000\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[170], line 8\u001b[0m, in \u001b[0;36mTrainNetworks\u001b[1;34m(networks, epochs, lams)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[39mfor\u001b[39;00m lam \u001b[39min\u001b[39;00m lams:\n\u001b[0;32m      7\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mLambda:\u001b[39m\u001b[39m\"\u001b[39m, lam, end\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m... \u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m     network\u001b[39m.\u001b[39;49mtrain(X_train, y_train, epochs, lam, X_val, y_val)\n\u001b[0;32m      9\u001b[0m     acc \u001b[39m=\u001b[39m network\u001b[39m.\u001b[39maccuracy(X_val, y_val)\n\u001b[0;32m     10\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mAccuracy:\u001b[39m\u001b[39m\"\u001b[39m, acc)\n",
      "Cell \u001b[1;32mIn[167], line 48\u001b[0m, in \u001b[0;36mNeuralNetwork.train\u001b[1;34m(self, X, y, epochs, lam, X_val, y_val)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[0;32m     47\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward(X)\n\u001b[1;32m---> 48\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbackward(y)\n\u001b[0;32m     49\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupdate(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlearningRate, lam)\n\u001b[0;32m     50\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcost_over_time_training\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcost(y, lam))\n",
      "Cell \u001b[1;32mIn[167], line 20\u001b[0m, in \u001b[0;36mNeuralNetwork.backward\u001b[1;34m(self, target)\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers[i]\u001b[39m.\u001b[39mbackward(target)\n\u001b[0;32m     19\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 20\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayers[i]\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayers[i\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m]\u001b[39m.\u001b[39;49md, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayers[i\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m]\u001b[39m.\u001b[39;49mw)\n",
      "Cell \u001b[1;32mIn[165], line 17\u001b[0m, in \u001b[0;36mHiddenLayer.backward\u001b[1;34m(self, nextLayerDelta, nextLayerWeights)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbackward\u001b[39m(\u001b[39mself\u001b[39m, nextLayerDelta, nextLayerWeights):\n\u001b[1;32m---> 17\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39md \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mdot(nextLayerDelta, nextLayerWeights) \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactFunc\u001b[39m.\u001b[39mf_prime(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mz)\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "layers = [[42, 30, 15, 15, 3], [42, 30, 15, 15, 15, 3], [42, 30, 15, 15, 15, 15, 3]]\n",
    "activations = [ActivationReLU(), ActivationSigmoid(), ActivationTanh()]\n",
    "initializations = [NormalInitialization]\n",
    "\n",
    "networks = []\n",
    "for layer in layers:\n",
    "    for activation in activations:\n",
    "        for initialization in initializations:\n",
    "            networks.append(BuildNetwork(layer, activation, initialization, 0.000005))\n",
    "\n",
    "network = TrainNetworks(networks, 1000)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
